import sys
import warnings
import traceback
import time
import json
import argparse
from pathlib import Path
from typing import Dict, List, Any, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed
from functools import partial
warnings.filterwarnings('ignore')

project_root = Path("../").resolve()
sys.path.append(str(project_root))

from pageindex import page_index_main, config

# é…ç½®æœ¬åœ°ç‰ˆæœ¬å‚æ•°
local_config = config(
    model='deepseek-chat',
    toc_check_page_num=20,
    max_page_num_each_node=10,
    max_token_num_each_node=20000,
    if_add_node_id='yes',
    if_add_node_summary='yes',
    if_add_doc_description='yes',
    if_add_node_text='yes'
)

print("âœ“ æœ¬åœ°ç‰ˆæœ¬é…ç½®å®Œæˆ")
print(f"æ¨¡å‹: {local_config.model}")


def process_document_local(pdf_path: Path, output_base_dir: Path = Path("./results"),
                          source_base_dir: Path = None) -> Dict[str, Any]:
    """ä½¿ç”¨æœ¬åœ°ç‰ˆæœ¬å¤„ç†å•ä¸ªPDFæ–‡æ¡£

    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        output_base_dir: è¾“å‡ºæ ¹ç›®å½•
        source_base_dir: æºæ–‡ä»¶æ ¹ç›®å½•ï¼Œç”¨äºè®¡ç®—ç›¸å¯¹è·¯å¾„
    """
    try:
        start_time = time.time()

        # å¤„ç†PDF
        tree_structure = page_index_main(str(pdf_path), local_config)

        # è®¡ç®—è¾“å‡ºè·¯å¾„ï¼Œä¿æŒç›®å½•ç»“æ„
        if source_base_dir:
            # è·å–ç›¸å¯¹äºæºç›®å½•çš„ç›¸å¯¹è·¯å¾„
            relative_path = pdf_path.relative_to(source_base_dir)
            # åœ¨è¾“å‡ºç›®å½•ä¸­é‡å»ºç›¸åŒçš„ç›®å½•ç»“æ„
            output_dir = output_base_dir / relative_path.parent
        else:
            output_dir = output_base_dir

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir.mkdir(parents=True, exist_ok=True)

        pdf_name = pdf_path.stem
        output_file = output_dir / f'{pdf_name}_structure.json'

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(tree_structure, f, indent=2, ensure_ascii=False)

        elapsed_time = time.time() - start_time

        return {
            'success': True,
            'file_name': pdf_path.name,
            'file_path': str(pdf_path),
            'tree': tree_structure,
            'output_file': str(output_file),
            'processing_time': elapsed_time
        }
    except Exception as e:
        tb = traceback.format_exc()
        return {
            'success': False,
            'file_name': pdf_path.name,
            'file_path': str(pdf_path),
            'error': str(e),
            'traceback': tb,
            'processing_time': 0
        }

def process_document_wrapper(pdf_file: Path, output_base_dir: Path, source_base_dir: Path) -> Dict[str, Any]:
    """åŒ…è£…å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œå¤„ç†"""
    return process_document_local(pdf_file, output_base_dir, source_base_dir)


def check_existing_indices(pdf_files: List[Path], output_base_dir: Path,
                          source_base_dir: Path) -> Dict[str, Any]:
    """æ£€æŸ¥å“ªäº›æ–‡ä»¶å·²æœ‰ç´¢å¼•ï¼Œå“ªäº›ç¼ºå¤±

    Args:
        pdf_files: PDFæ–‡ä»¶åˆ—è¡¨
        output_base_dir: è¾“å‡ºæ ¹ç›®å½•
        source_base_dir: æºæ–‡ä»¶æ ¹ç›®å½•

    Returns:
        åŒ…å«å·²å­˜åœ¨å’Œç¼ºå¤±ç´¢å¼•ä¿¡æ¯çš„å­—å…¸
    """
    existing = []
    missing = []

    for pdf_file in pdf_files:
        if source_base_dir:
            relative_path = pdf_file.relative_to(source_base_dir)
            output_dir = output_base_dir / relative_path.parent
        else:
            output_dir = output_base_dir
        output_file = output_dir / f'{pdf_file.stem}_structure.json'

        if output_file.exists():
            existing.append({
                'source': pdf_file,
                'index': output_file,
                'size': output_file.stat().st_size,
                'modified': output_file.stat().st_mtime
            })
        else:
            missing.append({
                'source': pdf_file,
                'index': output_file
            })

    return {
        'existing': existing,
        'missing': missing,
        'total': len(pdf_files)
    }


def batch_process_documents(pdf_files: List[Path], output_base_dir: Path,
                            source_base_dir: Path, max_workers: int = 4,
                            overwrite: bool = False) -> Dict[str, Any]:
    """å¹¶è¡Œæ‰¹é‡å¤„ç†æ–‡æ¡£

    Args:
        pdf_files: PDFæ–‡ä»¶åˆ—è¡¨
        output_base_dir: è¾“å‡ºæ ¹ç›®å½•
        source_base_dir: æºæ–‡ä»¶æ ¹ç›®å½•
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°
        overwrite: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„ç´¢å¼•æ–‡ä»¶ï¼ŒFalseè¡¨ç¤ºè·³è¿‡å·²å­˜åœ¨çš„
    """
    # è¿‡æ»¤å·²å¤„ç†çš„æ–‡ä»¶ï¼ˆå¦‚æœä¸è¦†ç›–ï¼‰
    if not overwrite:
        files_to_process = []
        skipped_count = 0
        for pdf_file in pdf_files:
            if source_base_dir:
                relative_path = pdf_file.relative_to(source_base_dir)
                output_dir = output_base_dir / relative_path.parent
            else:
                output_dir = output_base_dir
            output_file = output_dir / f'{pdf_file.stem}_structure.json'

            if output_file.exists():
                skipped_count += 1
            else:
                files_to_process.append(pdf_file)

        if skipped_count > 0:
            print(f"â­ è·³è¿‡å·²å­˜åœ¨ç´¢å¼•çš„æ–‡ä»¶: {skipped_count} ä¸ª")
        pdf_files = files_to_process
    else:
        print("âš ï¸  è¦†ç›–æ¨¡å¼ï¼šå°†é‡æ–°ç”Ÿæˆæ‰€æœ‰ç´¢å¼•æ–‡ä»¶")

    if not pdf_files:
        print("æ²¡æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶")
        return {
            'results': {},
            'total_time': 0,
            'success_count': 0,
            'total_count': 0
        }

    print(f"\nä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç† {len(pdf_files)} ä¸ªæ–‡ä»¶...")

    results = {}
    completed = 0
    start_time = time.time()

    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {
            executor.submit(process_document_wrapper, pdf_file, output_base_dir, source_base_dir): pdf_file
            for pdf_file in pdf_files
        }

        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_file):
            pdf_file = future_to_file[future]
            completed += 1

            try:
                result = future.result()
                results[pdf_file.name] = result

                if result['success']:
                    print(f"[{completed}/{len(pdf_files)}] âœ“ {pdf_file.name} - {result['processing_time']:.2f}ç§’")
                else:
                    print(f"[{completed}/{len(pdf_files)}] âœ— {pdf_file.name} - å¤±è´¥: {result['error']}")
            except Exception as e:
                print(f"[{completed}/{len(pdf_files)}] âœ— {pdf_file.name} - å¼‚å¸¸: {str(e)}")
                results[pdf_file.name] = {
                    'success': False,
                    'file_name': pdf_file.name,
                    'error': str(e),
                    'processing_time': 0
                }

    total_time = time.time() - start_time
    success_count = sum(1 for r in results.values() if r['success'])

    return {
        'results': results,
        'total_time': total_time,
        'success_count': success_count,
        'total_count': len(pdf_files)
    }


def main():
    """ä¸»å‡½æ•°ï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='å¹¶è¡Œå¤„ç†PDFæ–‡æ¡£ï¼Œç”Ÿæˆç´¢å¼•ç»“æ„',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æ£€æŸ¥ç´¢å¼•çŠ¶æ€
  python local_document_process.py --check

  # å¤„ç†ç¼ºå¤±ç´¢å¼•çš„æ–‡ä»¶ï¼ˆé»˜è®¤è·³è¿‡å·²å­˜åœ¨çš„ï¼‰
  python local_document_process.py

  # å¼ºåˆ¶è¦†ç›–æ‰€æœ‰ç´¢å¼•æ–‡ä»¶
  python local_document_process.py --overwrite

  # ä½¿ç”¨8ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç†
  python local_document_process.py -w 8

  # æŒ‡å®šè¾“å…¥è¾“å‡ºç›®å½•
  python local_document_process.py /path/to/docs -o /path/to/output
        """
    )
    parser.add_argument('input_dir', type=str, nargs='?',
                       default='çŸ¥è¯†åº“æµ‹è¯•é›†_pdf',
                       help='è¾“å…¥æ–‡æ¡£ç›®å½•è·¯å¾„')
    parser.add_argument('-o', '--output', type=str, default='./doc_index_results',
                       help='è¾“å‡ºç›®å½•è·¯å¾„ (é»˜è®¤: ./doc_index_results)')
    parser.add_argument('-w', '--workers', type=int, default=4,
                       help='å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•° (é»˜è®¤: 4)')
    parser.add_argument('-n', '--max-files', type=int, default=None,
                       help='é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡ï¼Œç”¨äºæµ‹è¯• (é»˜è®¤: å¤„ç†æ‰€æœ‰æ–‡ä»¶)')
    parser.add_argument('--overwrite', action='store_true',
                       help='è¦†ç›–å·²å­˜åœ¨çš„ç´¢å¼•æ–‡ä»¶ï¼ˆé»˜è®¤: è·³è¿‡å·²å­˜åœ¨çš„ï¼‰')
    parser.add_argument('--check', action='store_true',
                       help='åªæ£€æŸ¥ç´¢å¼•çŠ¶æ€ï¼Œä¸è¿›è¡Œå¤„ç†')
    parser.add_argument('-f', '--formats', type=str, nargs='+',
                       default=['.pdf', '.docx', '.doc'],
                       help='æ”¯æŒçš„æ–‡æ¡£æ ¼å¼ (é»˜è®¤: .pdf .docx .doc)')

    args = parser.parse_args()

    # é…ç½®å‚æ•°
    knowledge_base_path = Path(args.input_dir)
    output_base_dir = Path(args.output)
    max_workers = args.workers
    max_files = args.max_files
    overwrite = args.overwrite
    check_only = args.check
    supported_formats = args.formats

    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not knowledge_base_path.exists():
        print(f"âŒ é”™è¯¯: è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {knowledge_base_path}")
        return

    # æ‰«æçŸ¥è¯†åº“æ–‡æ¡£
    pdf_files = []
    for ext in supported_formats:
        pdf_files.extend(list(knowledge_base_path.rglob(f'*{ext}')))

    # æ’é™¤éšè—æ–‡ä»¶å’Œç³»ç»Ÿæ–‡ä»¶
    pdf_files = [f for f in pdf_files if not any(part.startswith('.') for part in f.parts)]

    print(f"âœ“ æ‰«æçŸ¥è¯†åº“å®Œæˆ")
    print(f"æ‰¾åˆ°æ–‡æ¡£æ•°é‡: {len(pdf_files)}")
    print(f"\næ–‡æ¡£æ ¼å¼åˆ†å¸ƒ:")
    for ext in supported_formats:
        count = sum(1 for f in pdf_files if f.suffix.lower() == ext)
        print(f"  {ext}: {count} ä¸ª")

    if len(pdf_files) > 0:
        print(f"\nå‰5ä¸ªæ–‡æ¡£ç¤ºä¾‹:")
        for i, doc in enumerate(pdf_files[:5], 1):
            print(f"  {i}. {doc.name}")

    # é™åˆ¶å¤„ç†æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    if max_files is not None:
        pdf_files = pdf_files[:max_files]
        print(f"\nâš  é™åˆ¶å¤„ç†æ•°é‡: {len(pdf_files)} ä¸ªæ–‡ä»¶")

    if len(pdf_files) == 0:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡æ¡£")
        return

    # æ£€æŸ¥æ¨¡å¼
    if check_only:
        print("\n" + "=" * 60)
        print("æ£€æŸ¥ç´¢å¼•çŠ¶æ€")
        print("=" * 60)

        check_result = check_existing_indices(pdf_files, output_base_dir, knowledge_base_path)

        print(f"\nğŸ“Š ç´¢å¼•çŠ¶æ€ç»Ÿè®¡:")
        print(f"  æ€»æ–‡æ¡£æ•°: {check_result['total']}")
        print(f"  å·²æœ‰ç´¢å¼•: {len(check_result['existing'])} ({len(check_result['existing'])/check_result['total']*100:.1f}%)")
        print(f"  ç¼ºå¤±ç´¢å¼•: {len(check_result['missing'])} ({len(check_result['missing'])/check_result['total']*100:.1f}%)")

        if check_result['existing']:
            print(f"\nâœ“ å·²æœ‰ç´¢å¼•çš„æ–‡ä»¶ ({len(check_result['existing'])} ä¸ª):")
            for i, item in enumerate(check_result['existing'][:10], 1):
                size_kb = item['size'] / 1024
                print(f"  {i}. {item['source'].name} ({size_kb:.1f} KB)")
            if len(check_result['existing']) > 10:
                print(f"  ... è¿˜æœ‰ {len(check_result['existing']) - 10} ä¸ªæ–‡ä»¶")

        if check_result['missing']:
            print(f"\nâœ— ç¼ºå¤±ç´¢å¼•çš„æ–‡ä»¶ ({len(check_result['missing'])} ä¸ª):")
            for i, item in enumerate(check_result['missing'][:10], 1):
                print(f"  {i}. {item['source'].name}")
            if len(check_result['missing']) > 10:
                print(f"  ... è¿˜æœ‰ {len(check_result['missing']) - 10} ä¸ªæ–‡ä»¶")

        print(f"\nğŸ’¡ æç¤º:")
        if check_result['missing']:
            print(f"  è¿è¡Œ 'python {sys.argv[0]}' å¤„ç†ç¼ºå¤±ç´¢å¼•çš„æ–‡ä»¶")
        if check_result['existing']:
            print(f"  è¿è¡Œ 'python {sys.argv[0]} --overwrite' é‡æ–°ç”Ÿæˆæ‰€æœ‰ç´¢å¼•")

        return

    # å¹¶è¡Œæ‰¹é‡å¤„ç†
    print("=" * 60)
    print("å¼€å§‹å¹¶è¡Œæ–‡æ¡£å¤„ç†")
    print("=" * 60)

    batch_result = batch_process_documents(
        pdf_files,
        output_base_dir,
        knowledge_base_path,
        max_workers=max_workers,
        overwrite=overwrite
    )

    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("å¤„ç†å®Œæˆï¼")
    print("=" * 60)
    print(f"æˆåŠŸ: {batch_result['success_count']} / {batch_result['total_count']}")
    print(f"æ€»è€—æ—¶: {batch_result['total_time']:.2f}ç§’")
    if batch_result['success_count'] > 0:
        avg_time = sum(r['processing_time'] for r in batch_result['results'].values() if r['success']) / batch_result['success_count']
        print(f"å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’/æ–‡æ¡£")
        print(f"è¾“å‡ºç›®å½•: {output_base_dir.resolve()}")

    # è¾“å‡ºå¤±è´¥çš„æ–‡æ¡£
    failed_docs = [name for name, r in batch_result['results'].items() if not r['success']]
    if failed_docs:
        print(f"\nå¤±è´¥çš„æ–‡æ¡£ ({len(failed_docs)}):")
        for doc_name in failed_docs:
            result = batch_result['results'][doc_name]
            print(f"  - {doc_name}: {result.get('error', 'Unknown error')}")


if __name__ == "__main__":
    main()